
title: Try Google's newly announced PaLM API with Weaviate

We are thrilled to announce two brand new Weaviate modules that will help you to get the most out Google's new PaLM large language model (LLM).

These new modules are:
- [`text2vec-palm`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-palm) for generating vector embeddings and running semantic (`nearText`) queries, and
- [`generative-palm`](/developers/weaviate/modules/reader-generator-modules/generative-palm) for generative search.

<!-- TODO - check Weaviate version number before merging. -->
These modules are available to all Weaviate users as of **today**, with the release of Weaviate version `v1.19.1`. They integrate the new Vertex PaLM API with Weaviate, allowing you to hit the ground running straight away with the latest in LLM and vector database technologies.

## <i class="fa-solid fa-face-thinking"></i> What is PaLM?

The Pathways Language Model (or `PaLM`) is Googleâ€™s own LLM. According to Google, PaLM was designed to generalize across domains and tasks while being highly efficient.

You can read more about in [this blog](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) or [this paper](https://arxiv.org/abs/2204.02311), but some of the highlights from these articles are that PaLM:
- Contains 540-billion ðŸ¤¯ parameters,
- Is a dense decoder-only Transformer model, and
- Was trained using a combination of English and multilingual datasets.

Don't take our word for it - take a look at these [demo snippets from Google's blog](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).

![Image](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhLmQjS3gOQ2x7ru3xovYjVw-Yr2fKDCqhDHByQZitD92Yu4L-v2BBa5f_VMfpWM4D0930Dmk35EY1TqGrYUtMQqJO41hkLqXuu51eOpXZ3PvYPSjf5stfEJNJn2idWnRYCCEgBiJuLDTXX5Fgt-Mk13kCKdO12JShGvDO_cArtLKv8U8obJaHiL5ASQg/s16000/Big%20Bench%20Sped%20Up%20Cropped.gif)

> Edit: We now have more details on the PaLM 2 family of models, which the API is based on! [Read about it here](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/). In the blog, Google especially highlights improvements in multilingual, reasoning and coding capabilities.

### <i class="fa-solid fa-screwdriver-wrench"></i> What can the Vertex PaLM API do?

Being an LLM, PaLM can perform numerous tasks, from question-answering, sentence-completion, reading comprehension tasks, common-sense reasoning tasks and more.

Additionally, it can perform multi-lingual NLP tasks reflecting its dataset. What's more, you can use Google's Vertex AI PaLM API to test, customize, and deploy instances of Google's LLMs.

In other words, you can not only use PaLM off-the-shelf, but also customize the foundation model to suit **your** needs.

## PaLM <i class="fa-solid fa-handshake"></i> Weaviate

![You can use Weaviate with the Vertex PALM API](./img/highlight.png)

And we are excited to tell you about the ['text2vec-palm`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-palm) and ['generative-palm'](/developers/weaviate/modules/reader-generator-modules/generative-palm) modules. These modules will help you to bring the power of the newest LLM on the block to the, *ahem*, `PaLM` of your hand - or at least, to your Weaviate instance.

### <i class="fa-solid fa-input-numeric"></i> Embeddings with `text2vec-palm`

The [`text2vec-palm`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-palm) module can use a PaLM model to convert text to vector embeddings that represent their meaning.

So, you can use the `text2vec-palm` module to build a Weaviate database using PaLM-derived vectors for text objects, and perform lightning-fast searches.

The PaLM model used, `textembedding-gecko-001`, takes a maximum of 3,072 input tokens, and outputs 768-dimensional vector embeddings.

Please note that according to Google, the embedding endpoint does *not* support fine-tuning.

### <i class="fa-solid fa-typewriter"></i> Generative search with `generative-palm`

The [`generative-palm`](/developers/weaviate/modules/reader-generator-modules/generative-palm) module is another exciting development for Weaviate and Google Cloud / Vertex AI users.

Our generative search modules work in two stages. First, a search is performed in Weaviate, and then a generative model such as PaLM is called to action, whereby the results are sent to prompt the LLM (PaLM in this case).

In other words - Weaviate can not only find the right data object for you, it can find it, and send it to PaLM to answer questions about it, expand upon it, summarize it, or otherwise transform it as you see fit.

And remember that the Vertex PaLM API allows further fine-tuning of your PaLM model? It means you can do all this with an LLM that has been tuned to your use case, such as your task, or domain.

The base PaLM model for generative search is `text-bison-001`, which has the following properties:

- Max input token: 8,192
- Max output tokens: 1,024
- Training data: Up to Feb 2023

## <i class="fa-solid fa-code"></i> Want to try it out?

You can read more about how to use these modules in our documentation! See the pages for:

- [`text2vec-palm`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-palm), and
- [`generative-palm`](/developers/weaviate/modules/reader-generator-modules/generative-palm).

import WhatNext from '/_includes/what-next.mdx'

<WhatNext />

