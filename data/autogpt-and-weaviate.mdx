
title: Giving Auto-GPT Long-Term Memory with Weaviate


As generative language models such as GPT-4 continue to push the boundaries of what AI can do, the excitement surrounding its potential is spreading quickly. Many applications and projects are built on top of GPT-4 to extend its capabilities and features. Additionally, many tools were created in order to interact with large language models, like [LangChain](https://python.langchain.com/en/latest/) as an example. [Auto-GPT](https://github.com/Torantulino/Auto-GPT) is one of the fastest rising open-source python projects harnessing the power of GPT-4!

## What is Auto-GPT?
Auto-GPT has gotten a lot of attention lately, with the amount of stars jumping from 20k to 80k in a matter of days. Auto-GPT chains together “thoughts” and completes various tasks or assignments **autonomously**. It takes GPT-4 one step further by enabling the model to run iteratively and complete various tasks in a siloed fashion. It can [write code and execute python scripts](https://twitter.com/SigGravitas/status/1642181498278408193?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1642181498278408193%7Ctwgr%5Eaa2e51a2fc46f95cf982d6baa333a4ea14d1b264%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fcdn.embedly.com%2Fwidgets%2Fmedia.html%3Ftype%3Dtext2Fhtmlkey%3Da19fcc184b9711e1b4764040d3dc5c07schema%3Dtwitterurl%3Dhttps3A%2F%2Ftwitter.com%2FSigGravitas%2Fstatus%2F1642181498278408193image%3Dhttps3A%2F%2Fi.embed.ly%2F1%2Fimage3Furl3Dhttps253A252F252Fabs.twimg.com252Ferrors252Flogo46x38.png26key3Da19fcc184b9711e1b4764040d3dc5c07), [conduct market research](https://twitter.com/BoucherNicolas/status/1646250166834307072), and even [order pizza](https://twitter.com/Saboo_Shubham_/status/1646739277328314368).

ChatGPT requires humans to prompt the large language model (LLM) by developing and refining the text prompt. Meaning you need to build out your request step-by-step in order for the LLM to “understand”. Auto-GPT on the other hand, is able to independently define objectives needed to complete the assigned task **without (or with reduced) human feedback and intervention**. This is because of its ability to chain together thoughts.

Chain of thought is a method that is used to help language models improve their reasoning. It breaks down tasks into the intermediate steps that are needed in order to complete it. The program will then run continuously until it completes these tasks. For example, if it is working on a coding project, it will debug the code as it goes.

Let’s dig a little deeper into how Auto-GPT works under the hood. At the time of writing, Auto-GPT uses GPT-4 (or optionally, GPT-3.5) for text generation and GPT-3.5 for file storage and summarization. At configuration, Auto-GPT is given a list of tools such as a code executor, google search API, or a calculator. Additionally, it is possible to give Auto-GPT access to long-term memory via a vector database, such as [Weaviate](https://weaviate.io/). Auto-GPT also has access to `skills` which are manifested as pre-configured prompts such as summarization.

Armed with these tools, Auto-GPT begins with a user query. For example, “Please write out a grocery list and create a recipe using each ingredient.” Auto-GPT takes this task and proposes an action plan to achieve the task such as:

Task: Write out a grocery list based on past items

Plan:
1. Use the long-term memory from the Weaviate database to curate the list from last week
2. Order the groceries
3. Use the internet to look up different recipes

Action:
1. View previous grocery lists
2. Order the groceries online
3. Use the internet to gather recipes

Since Auto-GPT is able to form these action plans autonomously, it is important that it confirms each action was completed. It shouldn’t jump to step 2, if step 1 isn’t finished. It will do this by reasoning with its actions. From there it will review the results and make a refined plan. The ability to reason and refine its actions is what makes Auto-GPT so clever.

## Examples of Auto-GPT
People all over Twitter have shared multiple demos of what they’ve built with Auto-GPT. The possibilities are endless! In this section, we will cover a few popular examples.

[Sully](https://twitter.com/SullyOmarr/status/1645205292756418562) shared a thread on using Auto-GPT to conduct market research. The task is to understand the space of waterproof shoes and find 5 competitors. It will then need to report the pros and cons of each.

[Varun Mayya](https://twitter.com/VarunMayya/status/1643902198164717569) pointed out the opportunity of having Auto-GPT running autonomously. It was tasked to create an app, however, it noticed Node wasn’t installed so it took it upon itself to download it by finding a Stack Overflow link.

[Shubham Saboo](https://twitter.com/Saboo_Shubham_/status/1646739277328314368) shared this video of Auto-GPT ordering pizza on Domino's website.

![autogpt and weaviate](./img/Weaviate-auto-gpt-dark.png#gh-dark-mode-only)
![autogpt and weaviate](./img/Weaviate-auto-gpt-light.png#gh-light-mode-only)

## How to use it in Weaviate

Auto-GPT has both a short-term and long-term memory. By connecting to a vector database, like Weaviate, you enable the application to retrieve specific data. This extension is great if you’re asking Auto-GPT to complete a task that it wasn’t trained on. For example, if you have data about your customer base in Boston and would like to form an ad, Auto-GPT wouldn’t be able to do that because this wasn’t included in the training data. The solution to this problem is to connect to your Weaviate instance so that Auto-GPT can fetch the necessary information. Additionally, Auto-GPT can save and recall its actions for future use.

Here is the [codebase](https://github.com/Significant-Gravitas/Auto-GPT/blob/v0.3.1/autogpt/memory/weaviate.py) to see how Weaviate is integrated in Auto-GPT.

The easiest way to use Weaviate with Auto-GPT is with a [WCS](https://console.weaviate.cloud) instance. Create a Sandbox instance by following [these steps](https://weaviate.io/developers/wcs/quickstart#create-a-weaviate-cluster), and install Auto-GPT using the latest instructions from the repo, with the following notes in mind.

Notes:
- At the time of writing, it suggested using the [latest stable release](https://github.com/Significant-Gravitas/Auto-GPT/releases/latest) only, rather than the `master` branch.
- We recommend running Auto-GPT in a Docker container so that it runs in a safer, sandboxed environment.
- If you intend to run Auto-GPT directly from your device, rather than in a Docker container, we suggest installing the required libraries into a virtual environment, rather than to your system Python.

During installation, edit the below variables in the Auto-GPT `.env` file based on the below:

```
MEMORY_BACKEND=weaviate  # Change from `local`
…

WEAVIATE_HOST=your-endpoint.weaviate.network  # URL to your Weaviate instance (without “https://”)
WEAVIATE_PROTOCOL=https  # http if deploying Weaviate locally with Docker
WEAVIATE_API_KEY=  # Add the API key here if you have authentication enabled
```

And then, you can start Auto-GPT. You can run it directly on your device, or within a Docker container. To run it directly, run:
`python -m autogpt`

Or run the following to start it within a Docker container:

```
docker build -t autogpt .
docker run -it --env-file=./.env -v $PWD/auto_gpt_workspace:/app/auto_gpt_workspace autogpt
```

The above steps will allow Auto-GPT to use the WCS instance as the memory backend, allowing it to store and retrieve information as required.

We note using a local instance of Weaviate with Docker is a little more complicated. You will need to modify the `docker-compose` file such that the Weaviate container and the Auto-GPT container are able to communicate with each other.

## Proceed with Caution

Although Auto-GPT is a great project to try, it is best to test it out with caution. It is currently in the experimental phase, so it is best to not use it in production. Additionally, allowing Auto-GPT to run for a long period of time could get very expensive, and keep in mind that it may affect your device. Such as by writing or modifying files, and installing dependencies for example. If your task requires multiple steps, it is best to keep an eye on your OpenAI API usage, such as by setting a limit on your spend. Check out the disclaimer [here](https://github.com/Significant-Gravitas/Auto-GPT#-disclaimer).


import WhatNext from '/_includes/what-next.mdx'

<WhatNext />
